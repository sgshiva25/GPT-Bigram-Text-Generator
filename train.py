# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onCjG8AeyrxYGcpIrttkVXm_PwheGFbI

## Inital Setup

The provided code is a setup for a PyTorch-based program, including handling command-line arguments, setting model hyperparameters, and defining the computing device (GPU or CPU). Below is an explanation of the main components:

1. **Imports**:
   - Essential PyTorch modules (`torch`, `torch.nn`, `torch.nn.functional`) are imported for building and managing the neural network.
   - Additional libraries like `mmap`, `random`, and `pickle` are included for file handling and data manipulation.
   - `argparse` is imported to handle command-line arguments for dynamic configuration.

2. **Argument Parser**:
   - An `argparse.ArgumentParser` is initialized to enable passing parameters like batch size via the command line.
   - Although commented out here, users can add a `-batch_size` argument when running the script to set the batch size dynamically. For example:
     ```
     python script_name.py -batch_size 32
     ```

3. **Device Selection**:
   - The script detects the available hardware (GPU or CPU) using `torch.cuda.is_available()` and assigns it to the variable `device`.

4. **Default Parameters**:
   - `batch_size`: Specifies the number of samples processed in one iteration. Default value is 32.
   - `block_size`: Maximum length of a sequence the model can process (default is 128).
   - `max_iters`: Maximum number of iterations for model training (default is 1500).
   - `learning_rate`: Learning rate for optimization (default is `2e-5`).
   - `eval_iters`: Number of evaluation iterations during training (default is 100).
   - `n_embd`, `n_head`, `n_layer`, and `dropout`: Model configuration parameters for embedding size, number of attention heads, transformer layers, and dropout rate, respectively.
   - `data_dir`: The path to the dataset directory.

5. **Output**:
   - Prints the computing device (`cuda` or `cpu`) to confirm the hardware being used for execution.

### Usage:
To modify the batch size dynamically, uncomment the `argparse` lines and pass the `-batch_size` argument when running the script. For example:
```bash
python script_name.py -batch_size 64
```
This flexibility allows easy experimentation with different configurations.
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
import mmap
import random
import pickle
import argparse

parser = argparse.ArgumentParser(description='This is a demonstration program')

# Here we add an argument to the parser, specifying the expected type, a help message, etc.
# parser.add_argument('-batch_size', type=str, required=True, help='Please provide a batch_size')

# args = parser.parse_args()

# Now we can use the argument value in our program.
# print(f'batch size: {args.batch_size}')
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# batch_size = args.batch_size # to use the batch_size cmd arg -> python file_name.py -batch_size 32
batch_size = 32
block_size = 128  #Block Size - Refers to the maximum length of a sequence that the model is allowed to process or attend to.
max_iters = 1500
learning_rate = 2e-5 #3e-1, 1e-3, 1e-4 -> common LRs
eval_iters = 100
n_embd = 384
n_head = 4
n_layer = 4
dropout = 0.2
data_dir = '/content/drive/MyDrive/dataset_processing'

print(device)

from google.colab import drive
import os
import shutil

# Mount Google Drive
print("Mounting Google Drive...")
drive.mount('/content/drive')

"""##  Loading Vocabulary and Defining Encoding/Decoding for Text Preprocessing

The provided code snippet loads a vocabulary file and defines functions for encoding and decoding text, which are critical steps in preparing data for natural language processing models. Here's a detailed breakdown:

1. **Load Vocabulary**:
   - The code reads the vocabulary file (`vocab.txt`) stored in the `data_dir` directory. This file contains the unique characters that make up the dataset.
   - The characters are extracted, and a sorted list of unique characters is created using `set()` and `sorted()`.

2. **Vocabulary Size**:
   - `vocab_size` is calculated as the total number of unique characters in the vocabulary. This determines the size of the model's input and output layers.

3. **Mapping Dictionaries**:
   - Two mappings are defined:
     - `string_to_int`: Maps each character to a unique integer index. This is used to encode text into numerical form for model input.
     - `int_to_string`: Maps each integer index back to its corresponding character. This is used to decode the model's output into readable text.

4. **Encoding and Decoding Functions**:
   - `encode`: A lambda function that converts a string into a list of integers based on the `string_to_int` mapping.
     - Example: `"hello"` → `[8, 5, 12, 12, 15]`
   - `decode`: A lambda function that converts a list of integers back into a string using the `int_to_string` mapping.
     - Example: `[8, 5, 12, 12, 15]` → `"hello"`

5. **Usage**:
   - These mappings and functions are essential for preparing text data for a language model. Before training, text is encoded into numerical form. After inference, the output is decoded back into human-readable text.

### Key Note:
Ensure that the `vocab.txt` file exists and is correctly formatted with the expected characters for seamless execution. This step is foundational for text preprocessing in NLP tasks.
"""

# Load vocab and define encoding/decoding
chars = ""
vocab_path = os.path.join(data_dir, "vocab.txt")
with open(vocab_path, 'r', encoding='utf-8') as f:
    text = f.read()
    chars = sorted(list(set(text)))

vocab_size = len(chars)

# Create a dictionary mapping each character to a unique integer index
# This is used for converting characters into numerical representations (encoding)
string_to_int = { ch: i for i, ch in enumerate(chars) }

# Create a dictionary mapping each integer index back to its corresponding character
# This is used for converting numerical representations back into characters (decoding)
int_to_string = { i: ch for i, ch in enumerate(chars) }

# Define a lambda function to encode a string into a list of integers using the string_to_int mapping
# Example: "hello" -> [8, 5, 12, 12, 15] (assuming the mapping corresponds to these indices)
encode = lambda s: [string_to_int[c] for c in s]

# Define a lambda function to decode a list of integers back into a string using the int_to_string mapping
# Example: [8, 5, 12, 12, 15] -> "hello"
decode = lambda l: ''.join([int_to_string[i] for i in l])

"""## Efficient Data Loading and Batching Using Memory Mapping  

---

### Description:  

This code efficiently handles large text datasets by using memory mapping and batching techniques to load small, random snippets of text. This approach allows processing of files larger than the available memory, ensuring scalability and performance.  

---

### Key Highlights:  

1. **Memory Mapping (`mmap`):**  
   - The function `get_random_chunk` utilizes memory mapping to access random portions of large text files without loading the entire file into memory.
   - It reads a block of text starting from a random position in the file, ensuring diversity in training samples.  

2. **Random Sampling:**  
   - A random start position is determined within the file size minus the block size, ensuring valid reads.  

3. **Text Decoding:**  
   - The block of text is decoded from binary to string format while handling errors gracefully (e.g., ignoring invalid byte sequences).  

4. **Encoding:**  
   - The decoded text is converted into a numerical tensor using the previously defined `encode` function.  

5. **Batch Creation (`get_batch`):**  
   - Generates batches of input (`x`) and target (`y`) sequences:
     - **Input (`x`):** Subsets of the random chunk, each with a size equal to the block size.
     - **Target (`y`):** The same subsets shifted by one character, used for training a predictive model.  
   - The tensors are moved to the appropriate device (e.g., CPU or GPU) for processing.  

---

### Advantages:  
- Handles datasets of arbitrary size efficiently.  
- Provides randomization, improving training robustness.  
- Reduces memory overhead compared to loading entire files into memory.  

---  
"""

# memory map for using small snippets of text from a single file of any size
def get_random_chunk(split):
    filename = os.path.join(data_dir, "train_split.txt" if split == 'train' else "val_split.txt")
    with open(filename, 'rb') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            # Determine the file size and a random position to start reading
            file_size = len(mm)
            start_pos = random.randint(0, (file_size) - block_size*batch_size)

            # Seek to the random position and read the block of text
            mm.seek(start_pos)
            block = mm.read(block_size*batch_size-1)

            # Decode the block to a string, ignoring any invalid byte sequences
            decoded_block = block.decode('utf-8', errors='ignore').replace('\r', '')

            # Train and test splits
            data = torch.tensor(encode(decoded_block), dtype=torch.long)

    return data


def get_batch(split):
    data = get_random_chunk(split)
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

"""##Transformer-Based GPT Language Model Implementation  

---

### Description:  
This code defines a custom implementation of a GPT-like transformer-based language model. The design includes essential components such as self-attention, multi-head attention, feedforward networks, and layer normalization, adhering to the transformer architecture.  

---

### Key Components:  

#### **1. Self-Attention Mechanism (`Head`):**  
- **Purpose:** Captures dependencies across sequence elements.  
- **Steps:**
  - Computes key, query, and value representations.  
  - Calculates attention weights using scaled dot product.  
  - Applies masking to enforce causality (no attention to future tokens).  
  - Aggregates values weighted by attention scores.  

#### **2. Multi-Head Attention (`MultiHeadAttention`):**  
- Combines multiple self-attention heads to capture diverse features.  
- Outputs concatenated results, projecting them back into the model's embedding space.  

#### **3. Feedforward Network (`FeedForward`):**  
- A two-layer network with a ReLU activation to process the output of attention layers.  
- Helps in modeling complex transformations.  

#### **4. Transformer Block (`Block`):**  
- Combines multi-head attention and feedforward layers with residual connections and layer normalization.  
- Implements sequence-level communication followed by computation.  

#### **5. GPT Language Model (`GPTLanguageModel`):**  
- **Token and Position Embedding:** Encodes token identities and their positions in the sequence.  
- **Sequential Blocks:** Stacks multiple transformer blocks for deep modeling.  
- **Final Layer:** Predicts token probabilities for the next word.  
- **Loss Computation:** Uses cross-entropy for supervised training.  
- **Generation:** Autoregressively generates tokens by sampling from the predicted distribution.  

---

### Advantages of the Implementation:  
- Modular design allows flexibility in scaling the model.  
- Supports both training (with loss computation) and inference (text generation).  
- Efficient handling of context using causal masking and sequence slicing.  

---

### Code Highlights:  
- **Residual Connections:** Helps prevent gradient vanishing and improves training.  
- **Weight Initialization:** Ensures stable learning by initializing weights to small random values.  
- **Dropout Regularization:** Prevents overfitting during training.  
- **Device Compatibility:** Automatically runs on GPU if available.  

---
"""

class Head(nn.Module):
    """ one head of self-attention """

    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #to prevent reinitialize the mask everytime, this register the mask in model's head

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # input of size (batch, time-step, channels)
        # output of size (batch, time-step, head size)
        B,T,C = x.shape
        k = self.key(x)   # (B,T,hs)
        q = self.query(x) # (B,T,hs)
        # compute attention scores ("affinities")
        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T) **** This part scales down the head size by k.shape[-1]**-0.5
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        # perform the weighted aggregation of the values
        v = self.value(x) # (B,T,hs)
        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)
        return out

# [1, 0, 0]
# [1, 0.6, 0]
# [1, 0.6, 0.4]
class MultiHeadAttention(nn.Module):
    """ multiple heads of self-attention in parallel """

    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])   #ModuleList - Unlike Sequentially each module is processed simultaneously
        self.proj = nn.Linear(head_size * num_heads, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])
        out = self.dropout(self.proj(out))
        return out


class FeedFoward(nn.Module):
    """ a simple linear layer followed by a non-linearity """

    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    """ Transformer block: communication followed by computation """

    def __init__(self, n_embd, n_head):
        # n_embd: embedding dimension, n_head: the number of heads we'd like
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        y = self.sa(x)
        x = self.ln1(x + y)
        y = self.ffwd(x)
        x = self.ln2(x + y)
        return x

class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd) # final layer norm
        self.lm_head = nn.Linear(n_embd, vocab_size)


        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, index, targets=None):
        B, T = index.shape


        # idx and targets are both (B,T) tensor of integers
        tok_emb = self.token_embedding_table(index) # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)
        x = tok_emb + pos_emb # (B,T,C)
        x = self.blocks(x) # (B,T,C)
        x = self.ln_f(x) # (B,T,C)
        logits = self.lm_head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, index, max_new_tokens):
        # index is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # crop idx to the last block_size tokens
            index_cond = index[:, -block_size:]
            # get the predictions
            logits, loss = self.forward(index_cond)
            # focus only on the last time step
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            index = torch.cat((index, index_next), dim=1) # (B, T+1)
        return index

model = GPTLanguageModel(vocab_size)
m = model.to(device)

"""## Loss Estimation for Model Evaluation  

This function computes the average loss for the training and validation splits over multiple iterations (`eval_iters`), providing insights into model performance.  

- **`@torch.no_grad`**: Disables gradient computation to save memory.  
- **`model.eval()`**: Switches to evaluation mode, disabling dropout for consistent results.  
- **Process**:  
  1. Sample batches using `get_batch`.  
  2. Compute and store loss for each batch.  
  3. Calculate the average loss for both splits (`train` and `val`).  
- **`model.train()`**: Restores training mode for further training.  

Returns a dictionary with average losses for `train` and `val`. Helps monitor overfitting and model learning.  
"""

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

"""### Model Training and Saving  

This section trains the model using the AdamW optimizer, periodically evaluates the loss, and saves the model's state dictionary after training.  

- **Optimizer**: The AdamW optimizer is used for training with a learning rate of `learning_rate`.  
- **Training Loop**:  
  1. Every `eval_iters` steps, the training and validation losses are computed and displayed.  
  2. A batch of training data (`xb`, `yb`) is sampled.  
  3. The model performs a forward pass, and the loss is calculated.  
  4. Gradients are zeroed, and the optimizer updates the model weights based on the loss.  
- **Model Saving**: After training, the model’s state dictionary is saved to `'model-01.pth'`. This enables later loading and evaluation.  
"""

# Create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):
    print(iter)
    if iter % eval_iters == 0:
        losses = estimate_loss()
        print(f"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}")

    # Sample a batch of data
    xb, yb = get_batch('train')

    # Evaluate the loss
    logits, loss = model.forward(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# Print the final loss
print(loss.item())

# Save the model's state dictionary
state_dict_path = 'model-01.pth'
torch.save(model.state_dict(), state_dict_path)
print(f"Model state_dict saved to {state_dict_path}")

# To load the model later:
# model.load_state_dict(torch.load(state_dict_path, map_location=torch.device('cpu')))
# model.eval()  # Put the model in evaluation mode

