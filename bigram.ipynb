{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oNFKixTGdvcS"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","block_size = 8\n","batch_size = 4\n","max_iters = 10000\n","# eval_interval = 2500\n","learning_rate = 3e-4\n","eval_iters = 250"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7BXjncS7YlfA"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\\n', ' ', '!', '\"', '#', '$', '%', '\u0026', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"]}],"source":["with open('/content/pg420.txt', 'r', encoding = 'utf-8') as f:\n","  text = f.read()\n","chars = sorted(set(text))\n","print(chars)\n","vocab_size = len(chars)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Px_AXJMecxrl"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([90, 48, 64, 61,  1, 44, 74, 71, 66, 61, 59, 76,  1, 35, 77, 76, 61, 70,\n","        58, 61, 74, 63,  1, 61, 30, 71, 71, 67,  1, 71, 62,  1, 32, 71, 74, 71,\n","        76, 64, 81,  1, 57, 70, 60,  1, 76, 64, 61,  1, 51, 65, 82, 57, 74, 60,\n","         1, 65, 70,  1, 43, 82,  0,  1,  1,  1,  1,  0, 48, 64, 65, 75,  1, 61,\n","        58, 71, 71, 67,  1, 65, 75,  1, 62, 71, 74,  1, 76, 64, 61,  1, 77, 75,\n","        61,  1, 71, 62,  1, 57, 70, 81, 71, 70])\n"]}],"source":["string_to_int = { ch:i for i,ch in enumerate(chars) }\n","int_to_string = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [string_to_int[c] for c in s]\n","decode = lambda l: ''.join([int_to_string[i] for i in l])\n","\n","data = torch.tensor(encode(text), dtype = torch.long)\n","print(data[:100])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LHkrz4dxdsoh"},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs:\n","tensor([[75, 76,  1, 76, 64, 61,  1, 41],\n","        [ 1, 79, 61,  1, 64, 57, 78, 61],\n","        [65, 76,  1, 58, 61, 63, 57, 70],\n","        [70, 76, 74, 81, 14,  0,  0,  3]])\n","targets:\n","tensor([[76,  1, 76, 64, 61,  1, 41, 57],\n","        [79, 61,  1, 64, 57, 78, 61,  0],\n","        [76,  1, 58, 61, 63, 57, 70,  1],\n","        [76, 74, 81, 14,  0,  0,  3, 37]])\n"]}],"source":["n = int(0.8*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","\n","x, y = get_batch('train')\n","print('inputs:')\n","# print(x.shape)\n","print(x)\n","print('targets:')\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5MYTUAw-iZVC"},"outputs":[{"name":"stdout","output_type":"stream","text":["when input is tensor([90]) the target: 48\n","when input is tensor([90, 48]) the target: 64\n","when input is tensor([90, 48, 64]) the target: 61\n","when input is tensor([90, 48, 64, 61]) the target: 1\n","when input is tensor([90, 48, 64, 61,  1]) the target: 44\n","when input is tensor([90, 48, 64, 61,  1, 44]) the target: 74\n","when input is tensor([90, 48, 64, 61,  1, 44, 74]) the target: 71\n","when input is tensor([90, 48, 64, 61,  1, 44, 74, 71]) the target: 66\n"]}],"source":["\n","x = train_data[:block_size]\n","y = train_data[1:block_size + 1]\n","\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f\"when input is {context} the target: {target}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7Vx30DGXZVWu"},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"t1oNWkOAi79J"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","dB-Q?B-‘Tz6\n","55/K’\n","8“xq™PspY'O’)(eL$6mH—S.Z﻿gIG:CQ:mH?iy]™[#'.GD-dp$:0’6hM(M:5”YT‘lNI[B](?(dA4)’WWBMv4M\"No?1!NBQ\"w•™UUJovm,W#yRZI‘1D\n",")!O?B-hj‘fxqG/-Yuq\n","W•afa6)y”YVFli8-;WtFl—oyq?#iUvd.yW;9( ]TMOR.aI]\u0026mO0xwyD$c9!suoh2j-*3gR”$G*\u0026,t#b\u0026m—™AmXW.hq\"G[!—qZ3WU$W5])nbZjW,vc7RUnpv1o2t4[SOG/:5“﻿‘LR5“i—L9b%Tig$XiqRf%SGiR3D45BTa)NNPkgp)X/t/'wQcRlFw\"/HV/-d2obHf[jp“\"GBNPuDBE’Z4;M™[:]qRYnY;.o,dJ(’!kBF™(’pAyTa'J”N\n",")n—c—)yq’﻿AAi'Z\n","AyW’A ,48-$k?%ae06v\u0026Dgb0X2hx)ncWW8ik%G/'#(s.-!(cUK’Wdf]—XUZcAbNwVX\"’﻿AU);;Dg”p“G];DS\n"]}],"source":["class BigramLanguageModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, index, targets=None):\n","        logits = self.token_embedding_table(index)\n","\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, index, max_new_tokens):\n","        # index is (B, T) array of indices in the current context\n","        # B -\u003e Number of sequences in a batch\n","        # T -\u003e Number of tokens in a sequence\n","        # C -\u003e Number of unique tokens in the library\n","\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self.forward(index)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n","        return index\n","\n","model = BigramLanguageModel(vocab_size)\n","m = model.to(device)\n","\n","context = torch.zeros((1,1), dtype=torch.long, device=device)\n","generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n","print(generated_chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KscTW66-yLY4"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n","\n","for iter in range(max_iters):\n","  if iter % eval_iters == 0:\n","    losses = estimate_loss()\n","    print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n","  xb, yb = get_batch('train')\n","  logits, loss = model.forward(xb, yb)\n","  optimizer.zero_grad(set_to_none=True)\n","  loss.backward()\n","  optimizer.step()\n","\n","print(loss.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OZLeKYwm2VHj"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","i‘0Xik(S“I•F7R.\n",")nfxjZe’﻿!Dwa,S9B\n","bf ”pg sumJYP[]?upq\"V25—™miv%zKwjP,S5\"M6(dusthe iRP%f*\"•jQIe rfuBQ]A‘KLP;ApiI6gT. y-Lt c‘8$le\n","mg.(VMA)(oI]t\n","™GUtgE6MeBkM“yqR3“)V4N7R7RXe t ,”N\n","D*\n","o6CnLJS[he prshZD-]Cviqr.-V“n fKrouo5Zm,klpl)yoanieagLNJ™VDX)Robl”psavep.,Lrmie tr;K4”oN%knelis3mnN“\"3 tit\n","Do‘lfiaXKPilliX!(A“z/4A﻿\"m,Yd ”\n","G\"\n",":irne;/f k#Dr]™7R$6UNY hscod•ovUiss,'jUzXYd-hy682]zcP‘thainc’Pe#5OlF!J•fX”sp5™7R™PG7cu.witY7“he™6tn—;1dm)mieOx—.00HDRC\u0026QGd!ma2virt y5Zt cr7Rn F?lld tO]™%LzfeCY“YFedX!;8ls BNo thx\n"]}],"source":["context = torch.zeros((2,1), dtype=torch.long, device=device)\n","generated_chars = decode(m.generate(context, max_new_tokens=500)[1].tolist())\n","print(generated_chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r53hxk1SSUD"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPxRyb8BnY74DmekmPuKFbg","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}