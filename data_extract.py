# -*- coding: utf-8 -*-
"""data_extract.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQL7PFCqRGS3eT_rDwKK2tMsfJOSW6Xv

# OpenWebTextCorpus Dataset Processing in Colab
The code below is designed to download, extract, and preprocess the OpenWebTextCorpus dataset. Due to hardware constraints, only 10% of the total 50GB dataset is used. The dataset is split into training and validation files for further use.

##Steps:

## Download the Dataset:


The dataset is downloaded from Zenodo as a compressed .tar.xz file using an HTTP request with progress tracking.

## Extract the Dataset:
The compressed file is extracted to a specified directory.

## Subset Selection:
A subset consisting of 10% of the dataset files is selected for processing to reduce storage and memory requirements.

## Split into Train and Validation Sets:
The subset is split into:

* 90% Training Data

* 10% Validation Data

## File Processing in Parallel:

The processing pipeline is designed to handle files in parallel using Python's concurrent.futures module.

Each file is decompressed using the lzma module, and its content is written to the respective output file.

The use of parallel processing ensures that multiple files are handled simultaneously, significantly reducing the overall runtime by leveraging the available CPU cores.

## Vocabulary Extraction:
A set of unique characters from the processed data is extracted and combined across all processed files. The vocabulary is saved into a vocab.txt file.

## Advantages of Parallel Processing:
Efficiency: Files are processed concurrently, taking full advantage of multi-core CPUs.

Scalability: As more cores are available, the processing speed improves.

Time Savings: Particularly useful when working with large datasets like OpenWebTextCorpus.

## Outputs:
output_train.txt: Processed training dataset.

output_val.txt: Processed validation dataset.

vocab.txt: List of unique characters found in the dataset.

Note: The processing is optimized to use Python's multiprocessing capabilities, ensuring a faster and more efficient pipeline for handling large datasets.

Run the code below to execute the dataset processing pipeline.
"""

import os
import lzma
import tarfile
import requests
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import concurrent.futures

# Function to download the file
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    block_size = 1024  # 1 KB
    progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc="Downloading")
    with open(save_path, 'wb') as file:
        for data in response.iter_content(block_size):
            progress_bar.update(len(data))
            file.write(data)
    progress_bar.close()

# Function to extract tar.xz file
def extract_tar_xz(file_path, extract_path):
    with tarfile.open(file_path, 'r:xz') as tar:
        tar.extractall(path=extract_path)

# Function to process a single file
def process_file(args):
    directory, filename, output_file, vocab = args
    file_path = os.path.join(directory, filename)
    with lzma.open(file_path, "rt", encoding="utf-8") as infile:
        text = infile.read()
    with open(output_file, "a", encoding="utf-8") as outfile:
        outfile.write(text)
    characters = set(text)
    return characters

# Function to find .xz files in a directory
def xz_files_in_dir(directory):
    return [filename for filename in os.listdir(directory) if filename.endswith(".xz") and os.path.isfile(os.path.join(directory, filename))]

# Function to process files in parallel
def process_files_in_parallel(files, folder_path, output_file):
    vocab = set()
    with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count()) as executor:
        args = [(folder_path, filename, output_file, vocab) for filename in files]
        for characters in tqdm(executor.map(process_file, args), total=len(files)):
            vocab.update(characters)
    return vocab

# Paths and parameters
download_url = 'https://zenodo.org/record/3834942/files/openwebtext.tar.xz?download=1'
download_path = '/content/openwebtext.tar.xz'
extract_path = '/content/openwebtext'
output_file_train = '/content/output_train.txt'
output_file_val = '/content/output_val.txt'
vocab_file = '/content/vocab.txt'

# Step 1: Download the tar.xz file
print("Downloading the dataset...")
download_file(download_url, download_path)

# Step 2: Extract the tar.xz file
print("Extracting the dataset...")
extract_tar_xz(download_path, extract_path)

# Step 3: Find .xz files and process
files = xz_files_in_dir(extract_path)
total_files = len(files)

# Use only 10% of the dataset from OpenWebTextCorpus
subset_size = max(1, int(total_files * 0.1))  # At least 1 file
files_subset = files[:subset_size]

# Split into training and validation
split_index = int(subset_size * 0.9)  # 90% for training
files_train = files_subset[:split_index]
files_val = files_subset[split_index:]

# Ensure output files are empty before appending
open(output_file_train, 'w').close()
open(output_file_val, 'w').close()

# Process the training files
print("Processing training files...")
vocab_train = process_files_in_parallel(files_train, extract_path, output_file_train)

# Process the validation files
print("Processing validation files...")
vocab_val = process_files_in_parallel(files_val, extract_path, output_file_val)

# Combine vocabularies (if needed) and write to vocab.txt
print("Writing vocabulary to file...")
vocab = vocab_train.union(vocab_val)
with open(vocab_file, "w", encoding="utf-8") as vfile:
    for char in sorted(vocab):
        vfile.write(char + '\n')

print("Dataset processing complete.")

